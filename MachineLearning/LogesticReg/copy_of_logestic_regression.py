# -*- coding: utf-8 -*-
"""Copy of Logestic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXOTPXZpWfnVKqT4RELgKAkkJTGhWHdn

##Logestic Regression

We need to find the sigmoid function to fit the data so we try to train the model with thetas is 0 then  we use the scipy gradient descent to find the values of the thetas which give as an small cost after we use the gradient descent we get accuracy 89% where the sigmoid is so close to real y value
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

data=pd.read_csv("/content/Data_for_LogReg.txt",names=["Exam1","Exam2","Admitted"],header=None)
print(data.head())
print(data.describe())

Admitted=data[data['Admitted'].isin([1])]
unAdmitted=data[data['Admitted'].isin([0])]

fig,ax = plt.subplots(figsize=(5,5))
ax.scatter(Admitted["Exam1"],Admitted["Exam2"],s=50, c='b', marker='o', label='Admitted')
ax.scatter(unAdmitted["Exam1"],unAdmitted["Exam2"],s=50, c='r', marker='x', label='unAdmitted')
ax.legend()
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')

def sigmoid(z):
  return 1/(1+np.exp(-z))

def cost(theta,x,y):
  theta=np.matrix(theta)
  x=np.matrix(x)
  y=np.matrix(y)
  f=np.multiply((-y),np.log(sigmoid(x*theta.T)))
  s=np.multiply((1-y),np.log(1-sigmoid(x*theta.T)))
  return (1/len(x))*np.sum(f-s)

data.insert(0,"ones",1)

data.head()

cols=data.shape[1]
x=data.iloc[:,0:cols-1]
y=data.iloc[:,cols-1:]
print("x:\n")
print(x)
print("Y:\n")
print(y)

x=np.array(x.values)
y=np.array(y.values)
theta=np.zeros(3)
print(theta)

print(cost(theta,x,y))

def gradient(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    
    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    
    error = sigmoid(X * theta.T) - y
    
    for i in range(parameters):
        term = np.multiply(error, X[:,i])
        grad[i] = np.sum(term) / len(X)
    
    return grad

import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(x, y))

print(result)

print(cost(result[0],x,y))

def perdict(theta,x):
  pr=sigmoid(x*theta.T)
  return[1 if i>=0.5 else 0 for i in pr]

new_theta=result[0]
new_theta=np.matrix(new_theta)
predaction=perdict(new_theta,x)
acc=[1 if(a==b==1) or (a==b==0) else 0 for(a,b) in zip(predaction,y)]
acc=sum(map(int, acc))/len(acc)
print(f"The Accuracy of the model is:{acc*100}%")